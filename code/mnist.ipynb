{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('py38_cv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ba15a1d979e8ac337d80e71852c3e63572e72a2fa439e8c45e7ad782aa6c2900"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### packages and globe settings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils import data\n",
    "from torchkeras import summary, Model\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "MNIST_ROOT = os.path.join('..', 'data')\n",
    "MNIST_PATH = os.path.join(MNIST_ROOT, 'MNIST')\n",
    "\n",
    "HISTORY_FILE = os.path.join(MNIST_PATH, 'mnist_history.csv')\n",
    "WEIGHT_FILE = os.path.join(MNIST_PATH, 'mnist_weight.pth')\n",
    "\n",
    "HISTORY1_FILE = os.path.join(MNIST_PATH, 'mnist_history1.csv')\n",
    "HISTORY2_FILE = os.path.join(MNIST_PATH, 'mnist_history2.csv')\n",
    "HISTORY3_FILE = os.path.join(MNIST_PATH, 'mnist_history3.csv')\n",
    "HISTORY4_FILE = os.path.join(MNIST_PATH, 'mnist_history4.csv')\n",
    "\n",
    "WEIGHT1_FILE = os.path.join(MNIST_PATH, 'mnist_weight1.pth')\n",
    "WEIGHT2_FILE = os.path.join(MNIST_PATH, 'mnist_weight2.pth')\n",
    "WEIGHT3_FILE = os.path.join(MNIST_PATH, 'mnist_weight3.pth')\n",
    "WEIGHT4_FILE = os.path.join(MNIST_PATH, 'mnist_weight4.pth')\n",
    "\n",
    "NB_CLASSES = 10\n",
    "NROWS = 8\n",
    "\n",
    "IMAGE_MEAN = 0.5\n",
    "IMAGE_STD = 0.5\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = 64\n",
    "LR = 1e-2"
   ]
  },
  {
   "source": [
    "### common codes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "def plot_metric(dfhistory, metric):\n",
    "    train_metrics = dfhistory[metric]\n",
    "    val_metrics = dfhistory['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'valid_'+metric])\n",
    "    plt.show()\n",
    "\n",
    "def plot_images(features, mean=0.5, std=0.5, nrows=8, figsize=(2, 2)):\n",
    "    # images: tensor (B, C, H, W), grid_image: ndarray (C, H, W)\n",
    "    grid_image = utils.make_grid(features, nrow=nrows).numpy()\n",
    "    grid_image = mean + grid_image * std\n",
    "\n",
    "    # imshow (H, W, C)\n",
    "    grid_image = grid_image.transpose(1, 2, 0)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(grid_image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "# save and load\n",
    "def save_history(model, file, mode='csv'):\n",
    "    assert mode == 'csv'\n",
    "    assert type(model.history) is pd.DataFrame\n",
    "    model.history.to_csv(file)\n",
    "\n",
    "def save_weight(model, file):\n",
    "    weights = dict()\n",
    "    weights.update({'epoch': model.epoch})\n",
    "    weights.update({'net': model.state_dict()})\n",
    "    weights.update({'optimizer': model.optim.state_dict()})\n",
    "    torch.save(weights, file)\n",
    "\n",
    "def load_history(file, index_col='epoch', mode='csv'):\n",
    "    assert mode == 'csv'\n",
    "    return pd.read_csv(file, index_col=index_col)\n",
    "\n",
    "def load_weight(model, file, net_only=False):\n",
    "    weights = torch.load(file)\n",
    "    model.load_state_dict(weights['net'])\n",
    "    if not net_only:\n",
    "        model.epoch = weights.get('epoch', 0)\n",
    "        model.optim.load_state_dict(weights['optimizer'])\n",
    "    return model\n",
    "\n",
    "# metrics\n",
    "def precision_metrics(targets, labels):\n",
    "    # targets (-1, C), labels (-1)\n",
    "    y_pred = targets.data.max(1)[1].numpy()\n",
    "    y_true = labels.numpy()\n",
    "    score = precision_score(y_true, y_pred, average='macro')\n",
    "    # return (1)\n",
    "    return torch.tensor(score)\n",
    "\n",
    "def accuracy_metrics(targets, labels):\n",
    "    # targets (-1, C), labels (-1)\n",
    "    y_pred = targets.data.max(1)[1].numpy()\n",
    "    y_true = labels.numpy()\n",
    "    score = accuracy_score(y_true, y_pred)\n",
    "    # return (1)\n",
    "    return torch.tensor(score)\n",
    "\n",
    "# training functions\n",
    "def run_step(model, features, labels, train_mode=True):\n",
    "    targets = model(features)\n",
    "    \n",
    "    metrics = dict()\n",
    "    loss = model.loss_fn(targets, labels)\n",
    "    metrics.update({'%sloss' % ('' if train_mode else 'val_'): loss.item()})\n",
    "    \n",
    "    for metric_name, metric_fn in model.metrics_dict.items():\n",
    "        metric_value = metric_fn(targets, labels)\n",
    "        metrics.update({'%s%s' % ('' if train_mode else 'val_', metric_name): metric_value.item()})\n",
    "\n",
    "    loss.backward()\n",
    "    model.optim.step()\n",
    "    model.optim.zero_grad()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def run_epoch(model, dataloader, train_mode=True, log_per_steps=200):\n",
    "    metrics_epoch = dict()\n",
    "\n",
    "    model.train(train_mode)\n",
    "    for step, (features, labels) in enumerate(dataloader, 1):\n",
    "        metrics = run_step(model, features, labels, train_mode)\n",
    "\n",
    "        # # update loss_epoch (mean)\n",
    "        # loss_epoch = (step - 1) / step * loss_epoch + metric_val / step\n",
    "        # update metric_epoch (mean)\n",
    "        for metric_name, metric_val in metrics.items():\n",
    "            if metrics_epoch.get(metric_name) == None:\n",
    "                metrics_epoch[metric_name] = metric_val\n",
    "            else:\n",
    "                metrics_epoch[metric_name] = \\\n",
    "                    (step - 1) / step * metrics_epoch[metric_name] + metric_val / step\n",
    "\n",
    "        if step % log_per_steps == 0:\n",
    "            print(\" - Step %d, %s\" % (step, metrics_epoch))\n",
    "\n",
    "    return metrics_epoch\n",
    "\n",
    "def train_model(model, dataloader_train, dataloader_valid, epochs, log_per_epochs=10, log_per_steps=200):\n",
    "    print(\"==========\" * 6)\n",
    "    print(\"= Training model\")\n",
    "    \n",
    "    metrics_list = []\n",
    "    start_epoch = 1 + model.epoch\n",
    "    end_epoch = epochs + 1 + model.epoch\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        metrics = dict()\n",
    "        print(\"==========\" * 6)\n",
    "        print(\"= Epoch %d/%d @ %s\" % (epoch, end_epoch - 1, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        metrics_train = run_epoch(model, dataloader_train, train_mode=True, log_per_steps=log_per_steps)\n",
    "        metrics_valid = run_epoch(model, dataloader_valid, train_mode=False, log_per_steps=log_per_steps)\n",
    "        metrics.update({'epoch': epoch})\n",
    "        metrics.update(metrics_train)\n",
    "        metrics.update(metrics_valid)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "        model.epoch = epoch\n",
    "\n",
    "        if epoch % log_per_epochs == 0:\n",
    "            print('= %s' % metrics)\n",
    "        \n",
    "    print(\"==========\" * 6)\n",
    "    \n",
    "    model.history = pd.DataFrame(metrics_list)\n",
    "    model.history.set_index('epoch', inplace=True)\n",
    "    return model.history\n",
    "\n",
    "def predict_model(model, features):\n",
    "    model.eval()\n",
    "    targets = model(features)\n",
    "    \n",
    "    return targets.data.max(1)[1]\n",
    "\n",
    "def eval_model(model, features, labels):\n",
    "    model.eval()\n",
    "    targets = model(features)\n",
    "\n",
    "    metrics = dict()\n",
    "    for metric_name, metric_fn in model.metrics_dict.items():\n",
    "        metric_value = metric_fn(targets, labels)\n",
    "        metrics.update({metric_name: metric_value.item()})\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "source": [
    "### datasets and dataloader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets and dataloader\n",
    "data_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 0~255 -> 0~1\n",
    "    transforms.Normalize((IMAGE_STD, ), (IMAGE_MEAN, ))  # 0~1 -> -1~1\n",
    "])\n",
    "\n",
    "ds_train = datasets.MNIST(MNIST_ROOT, train=True, transform=data_tf, download=True)\n",
    "ds_valid = datasets.MNIST(MNIST_ROOT, train=False, transform=data_tf, download=True)\n",
    "\n",
    "dl_train = data.DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dl_valid = data.DataLoader(ds_valid, batch_size=VAL_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "source": [
    "### batch sample plot (optional)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch sample plot\n",
    "batch_features, _ = next(iter(dl_train))\n",
    "\n",
    "plot_images(batch_features, mean=IMAGE_MEAN, std=IMAGE_STD, nrows=NROWS)"
   ]
  },
  {
   "source": [
    "### network class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, nb_classes=10, *args, **kwargs):\n",
    "        super(SimpleCNN, self).__init__(*args, **kwargs)\n",
    "        self.epoch = 0\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.dropout1 = nn.Dropout2d()\n",
    "        self.max_pool2 = nn.MaxPool2d(2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.flatten1 = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(50, nb_classes)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.logsoftmax1 = nn.LogSoftmax(1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.conv1(input)\n",
    "        input = self.max_pool1(input)\n",
    "        input = self.relu1(input)\n",
    "\n",
    "        input = self.conv2(input)\n",
    "        input = self.dropout1(input)\n",
    "        input = self.max_pool2(input)\n",
    "        input = self.relu2(input)\n",
    "        \n",
    "        input = self.flatten1(input)\n",
    "        input = self.fc1(input)\n",
    "        input = self.relu3(input)\n",
    "\n",
    "        input = self.fc2(input)\n",
    "        input = self.relu4(input)\n",
    "\n",
    "        input = self.logsoftmax1(input)\n",
    "\n",
    "        return input"
   ]
  },
  {
   "source": [
    "### network topology (optional)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network topology\n",
    "Model(SimpleCNN(NB_CLASSES)).summary(input_shape=(1, 28, 28))"
   ]
  },
  {
   "source": [
    "### model training and save history and weights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training settings (loss, optim & metrics)\n",
    "model = SimpleCNN(NB_CLASSES)\n",
    "model.loss_fn = nn.CrossEntropyLoss()\n",
    "model.optim = optim.Adam(model.parameters(), lr=LR)\n",
    "model.metrics_dict = {\n",
    "    'precision': precision_metrics,\n",
    "    'accuracy': accuracy_metrics\n",
    "}\n",
    "\n",
    "# model training\n",
    "history = train_model(model, dl_train, dl_valid, epochs=20, log_per_epochs=1, log_per_steps=200)\n",
    "\n",
    "# save training history\n",
    "save_history(model, HISTORY1_FILE)\n",
    "\n",
    "# save weights\n",
    "save_weight(model, WEIGHT1_FILE)"
   ]
  },
  {
   "source": [
    "### re-training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN(NB_CLASSES)\n",
    "model.loss_fn = nn.CrossEntropyLoss()\n",
    "model.optim = optim.Adam(model.parameters(), lr=LR)\n",
    "model.metrics_dict = {\n",
    "    'precision': precision_metrics,\n",
    "    'accuracy': accuracy_metrics\n",
    "}\n",
    "\n",
    "# load weights\n",
    "model = load_weight(model, WEIGHT1_FILE)\n",
    "history = train_model(model, dl_train, dl_valid, 10, log_per_epochs=1, log_per_steps=200)\n",
    "\n",
    "save_history(model, HISTORY2_FILE)\n",
    "save_weight(model, WEIGHT2_FILE)\n",
    "history"
   ]
  },
  {
   "source": [
    "### metrics plot"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = load_history(HISTORY_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history, 'precision')"
   ]
  },
  {
   "source": [
    "### prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, _ = next(iter(dl_valid))\n",
    "\n",
    "model = SimpleCNN(NB_CLASSES)\n",
    "model = load_weight(model, WEIGHT_FILE, net_only=True)\n",
    "\n",
    "targets = predict_model(model, features)\n",
    "print(targets.numpy().reshape(-1, NROWS))\n",
    "\n",
    "plot_images(features, mean=IMAGE_MEAN, std=IMAGE_STD, nrows=NROWS)"
   ]
  },
  {
   "source": [
    "### evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(iter(dl_valid))\n",
    "\n",
    "model = SimpleCNN(NB_CLASSES)\n",
    "model.metrics_dict = {\n",
    "    'precision': precision_metrics,\n",
    "    'accuracy': accuracy_metrics\n",
    "}\n",
    "model = load_weight(model, WEIGHT_FILE, net_only=True)\n",
    "\n",
    "metrics = eval_model(model, features, labels)\n",
    "\n",
    "# print(labels.reshape(-1, NROWS))\n",
    "# plot_images(features)\n",
    "print(metrics)"
   ]
  }
 ]
}