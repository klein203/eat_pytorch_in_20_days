{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('py38_cv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ba15a1d979e8ac337d80e71852c3e63572e72a2fa439e8c45e7ad782aa6c2900"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils import data\n",
    "from torchkeras import summary, Model\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "source": [
    "### common codes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric plot\n",
    "def plot_metric(dfhistory, metric):\n",
    "    train_metrics = dfhistory[metric]\n",
    "    val_metrics = dfhistory['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'valid_'+metric])\n",
    "    plt.show()\n",
    "\n",
    "# metrics\n",
    "def precision_metrics(targets, labels):\n",
    "    y_pred = targets.data.max(1)[1].numpy()\n",
    "    y_true = labels.numpy()\n",
    "    score = precision_score(y_true, y_pred, average='macro')\n",
    "    return torch.tensor(score)\n",
    "\n",
    "def accuracy_metrics(targets, labels):\n",
    "    y_pred = targets.data.max(1)[1].numpy()\n",
    "    y_true = labels.numpy()\n",
    "    score = accuracy_score(y_true, y_pred)\n",
    "    return torch.tensor(score)\n",
    "\n",
    "\n",
    "# training functions\n",
    "def run_step(model, features, labels, train_mode=True):\n",
    "    targets = model(features)\n",
    "    \n",
    "    metrics = dict()\n",
    "    loss = model.loss_fn(targets, labels)\n",
    "    metrics.update({'%sloss' % ('' if train_mode else 'val_'): loss.item()})\n",
    "    \n",
    "    for metric_name, metric_fn in model.metrics_dict.items():\n",
    "        metric_value = metric_fn(targets, labels)\n",
    "        metrics.update({'%s%s' % ('' if train_mode else 'val_', metric_name): metric_value.item()})\n",
    "\n",
    "    loss.backward()\n",
    "    model.optim.step()\n",
    "    model.optim.zero_grad()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def run_epoch(model, dataloader, train_mode=True, log_per_steps=200):\n",
    "    metrics_epoch = dict()\n",
    "\n",
    "    model.train(train_mode)\n",
    "    for step, (features, labels) in enumerate(dataloader, 1):\n",
    "        metrics = run_step(model, features, labels, train_mode)\n",
    "\n",
    "        # # update loss_epoch (mean)\n",
    "        # loss_epoch = (step - 1) / step * loss_epoch + metric_val / step\n",
    "        # update metric_epoch (mean)\n",
    "        for metric_name, metric_val in metrics.items():\n",
    "            if metrics_epoch.get(metric_name) == None:\n",
    "                metrics_epoch[metric_name] = metric_val\n",
    "            else:\n",
    "                metrics_epoch[metric_name] = \\\n",
    "                    (step - 1) / step * metrics_epoch[metric_name] + metric_val / step\n",
    "\n",
    "        if step % log_per_steps == 0:\n",
    "            print(\" - Step %d, %s\" % (step, metrics_epoch))\n",
    "\n",
    "    return metrics_epoch\n",
    "\n",
    "\n",
    "def train_model(model, dataloader_train, dataloader_valid, epochs, log_per_epochs=10, log_per_steps=200, skip_epoch=0):\n",
    "    print(\"==========\" * 6)\n",
    "    print(\"= Training model\")\n",
    "    \n",
    "    metrics_list = []\n",
    "    start_epoch = 1 + skip_epoch\n",
    "    end_epoch = epochs + 1 + skip_epoch\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        metrics = dict()\n",
    "        print(\"==========\" * 6)\n",
    "        print(\"= Epoch %d/%d @ %s\" % (epoch, epochs + skip_epoch, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        metrics_train = run_epoch(model, dataloader_train, train_mode=True, log_per_steps=log_per_steps)\n",
    "        metrics_valid = run_epoch(model, dataloader_valid, train_mode=False, log_per_steps=log_per_steps)\n",
    "        metrics.update({'epoch': epoch})\n",
    "        metrics.update(metrics_train)\n",
    "        metrics.update(metrics_valid)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "        if epoch % log_per_epochs == 0:\n",
    "            print('= %s' % metrics)\n",
    "        \n",
    "    print(\"==========\" * 6)\n",
    "    \n",
    "    history = pd.DataFrame(metrics_list)\n",
    "    history.set_index('epoch', inplace=True)\n",
    "    return history\n",
    "\n",
    "\n",
    "def predict_model(model, features):\n",
    "    targets = model(features).data.max(1)[1]\n",
    "    return targets\n",
    "\n",
    "\n",
    "def plot_images(images, nrows=8, figsize=(2, 2)):\n",
    "    # images (B, C, H, W)\n",
    "    grid_image = utils.make_grid(images, nrow=nrows)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(grid_image.numpy().transpose(1, 2, 0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "### datasets and dataloader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets and dataloader\n",
    "MNIST_PATH = os.path.join('..', 'data')\n",
    "HISTORY_FILE = os.path.join(MNIST_PATH, 'MNIST', 'mnist_history.csv')\n",
    "HISTORY2_FILE = os.path.join(MNIST_PATH, 'MNIST', 'mnist_history2.csv')\n",
    "WEIGHT_FILE = os.path.join(MNIST_PATH, 'MNIST', 'mnist_weight.pth')\n",
    "WEIGHT2_FILE = os.path.join(MNIST_PATH, 'MNIST', 'mnist_weight2.pth')\n",
    "\n",
    "NB_CLASSES = 10\n",
    "\n",
    "data_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 0~255 -> 0~1\n",
    "    transforms.Normalize((0.5, ), (0.5, ))  # 0~1 -> -1~1\n",
    "])\n",
    "\n",
    "ds_train = datasets.MNIST(MNIST_PATH, train=True, transform=data_tf, download=True)\n",
    "ds_valid = datasets.MNIST(MNIST_PATH, train=False, transform=data_tf, download=True)\n",
    "\n",
    "dl_train = data.DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "dl_valid = data.DataLoader(ds_valid, batch_size=64, shuffle=True)"
   ]
  },
  {
   "source": [
    "### batch sample plot (optional)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch sample plot\n",
    "batch_images, batch_labels = next(iter(dl_train))\n",
    "\n",
    "mean = 0.5\n",
    "std = 0.5\n",
    "batch_images = batch_images * std + mean\n",
    "\n",
    "plot_images(batch_images)"
   ]
  },
  {
   "source": [
    "### network class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, nb_classes=10, *args, **kwargs):\n",
    "        super(SimpleCNN, self).__init__(*args, **kwargs)\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.dropout1 = nn.Dropout2d()\n",
    "        self.max_pool2 = nn.MaxPool2d(2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.flatten1 = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(50, nb_classes)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.logsoftmax1 = nn.LogSoftmax(1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.conv1(input)\n",
    "        input = self.max_pool1(input)\n",
    "        input = self.relu1(input)\n",
    "\n",
    "        input = self.conv2(input)\n",
    "        input = self.dropout1(input)\n",
    "        input = self.max_pool2(input)\n",
    "        input = self.relu2(input)\n",
    "        \n",
    "        input = self.flatten1(input)\n",
    "        input = self.fc1(input)\n",
    "        input = self.relu3(input)\n",
    "\n",
    "        input = self.fc2(input)\n",
    "        input = self.relu4(input)\n",
    "\n",
    "        input = self.logsoftmax1(input)\n",
    "\n",
    "        return input"
   ]
  },
  {
   "source": [
    "### network topology (optional)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network topology\n",
    "Model(SimpleCNN(NB_CLASSES)).summary(input_shape=(1, 28, 28))"
   ]
  },
  {
   "source": [
    "### training settings (loss, optim & metrics)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN(NB_CLASSES)\n",
    "model.loss_fn = nn.CrossEntropyLoss()\n",
    "model.optim = optim.Adam(model.parameters(), lr=1e-3)\n",
    "model.metrics_dict = {\n",
    "    'precision': precision_metrics,\n",
    "    'accuracy': accuracy_metrics\n",
    "}"
   ]
  },
  {
   "source": [
    "### training and show history"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "dfhistory = train_model(model, dl_train, dl_valid, 10, log_per_epochs=1, log_per_steps=200)\n",
    "dfhistory"
   ]
  },
  {
   "source": [
    "### save history and weights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training history\n",
    "dfhistory.to_csv(HISTORY_FILE)\n",
    "\n",
    "# save weights\n",
    "torch.save(model.state_dict(), WEIGHT_FILE)"
   ]
  },
  {
   "source": [
    "### load training history (optional)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training history\n",
    "dfhistory = pd.read_csv(HISTORY_FILE, index_col='epoch')\n",
    "dfhistory"
   ]
  },
  {
   "source": [
    "### re-training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN(NB_CLASSES)\n",
    "model.loss_fn = nn.CrossEntropyLoss()\n",
    "model.optim = optim.Adam(model.parameters(), lr=1e-3)\n",
    "model.metrics_dict = {\n",
    "    'precision': precision_metrics,\n",
    "    'accuracy': accuracy_metrics\n",
    "}\n",
    "\n",
    "# load weights\n",
    "weights = torch.load(WEIGHT_FILE)\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "dfhistory2 = train_model(model, dl_train, dl_valid, 2, log_per_epochs=1, log_per_steps=200, skip_epoch=10)\n",
    "dfhistory2"
   ]
  },
  {
   "source": [
    "### save history and weights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training history\n",
    "dfhistory2.to_csv(HISTORY2_FILE)\n",
    "\n",
    "# save weights\n",
    "torch.save(model.state_dict(), WEIGHT2_FILE)"
   ]
  },
  {
   "source": [
    "### metrics plot"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(dfhistory2, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(dfhistory2, 'precision')"
   ]
  },
  {
   "source": [
    "### prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, _ = next(iter(dl_valid))\n",
    "\n",
    "model = SimpleCNN(NB_CLASSES)\n",
    "\n",
    "NROWS = 8\n",
    "\n",
    "# load weights\n",
    "weights = torch.load(WEIGHT2_FILE)\n",
    "model.load_state_dict(weights)\n",
    "targets = predict_model(model, samples)\n",
    "print(targets.numpy().reshape(-1, NROWS))\n",
    "\n",
    "mean = 0.5\n",
    "std = 0.5\n",
    "samples = samples * std + mean\n",
    "plot_images(samples, nrows=NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}