{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from torch.utils.data import TensorDataset,Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torch.utils.data.dataset.TensorDataset'>\n<class 'torch.utils.data.dataset.Subset'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "ds_iris = TensorDataset(torch.tensor(iris.data),torch.tensor(iris.target))\n",
    "\n",
    "nb_train = int(0.8 * len(ds_iris))\n",
    "nb_valid = len(ds_iris) - nb_train\n",
    "\n",
    "ds_train, ds_valid = random_split(ds_iris, [nb_train, nb_valid])\n",
    "\n",
    "print(type(ds_iris))\n",
    "print(type(ds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[4.6000, 3.2000, 1.4000, 0.2000],\n        [6.0000, 3.4000, 4.5000, 1.6000],\n        [6.2000, 2.2000, 4.5000, 1.5000],\n        [4.6000, 3.4000, 1.4000, 0.3000],\n        [6.7000, 3.0000, 5.2000, 2.3000],\n        [6.5000, 3.0000, 5.5000, 1.8000],\n        [7.7000, 2.8000, 6.7000, 2.0000],\n        [5.7000, 4.4000, 1.5000, 0.4000]], dtype=torch.float64) tensor([0, 1, 1, 0, 2, 2, 2, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=8)\n",
    "dl_valid = DataLoader(ds_valid, batch_size=8)\n",
    "\n",
    "for features, labels in dl_train:\n",
    "    print(features, labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "len(ds_train) =  120\nlen(ds_valid) =  30\nlen(ds_train+ds_valid) =  150\n<class 'torch.utils.data.dataset.ConcatDataset'>\n"
     ]
    }
   ],
   "source": [
    "ds_data = ds_train + ds_valid\n",
    "\n",
    "print('len(ds_train) =', len(ds_train))\n",
    "print('len(ds_valid) =', len(ds_valid))\n",
    "print('len(ds_train+ds_valid) =', len(ds_data))\n",
    "\n",
    "print(type(ds_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据结构\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import OrderedDict\n",
    "import re, string\n",
    "\n",
    "MAX_WORDS = 10000  # 仅考虑最高频的10000个词\n",
    "MAX_LEN = 200  # 每个样本保留200个词的长度\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "train_data_path = '../data/imdb/train.tsv'\n",
    "test_data_path = '../data/imdb/test.tsv'\n",
    "train_token_path = '../data/imdb/train_token.tsv'\n",
    "test_token_path = '../data/imdb/test_token.tsv'\n",
    "train_samples_path = '../data/imdb/train_samples/'\n",
    "test_samples_path = '../data/imdb/test_samples/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      count  word_id\n",
       "the  268230        2\n",
       "and  129713        3\n",
       "a    129479        4\n",
       "of   116497        5\n",
       "to   108296        6\n",
       "is    85615        7\n",
       "      84074        8\n",
       "in    74715        9\n",
       "it    62587       10\n",
       "i     60837       11"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>word_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>the</th>\n      <td>268230</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>129713</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>a</th>\n      <td>129479</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>of</th>\n      <td>116497</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>to</th>\n      <td>108296</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>is</th>\n      <td>85615</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>84074</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>in</th>\n      <td>74715</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>it</th>\n      <td>62587</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>i</th>\n      <td>60837</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "word_count_dict = dict()\n",
    "\n",
    "def clean_text(text):\n",
    "    lowercase = text.lower().replace(\"\\n\",\" \")\n",
    "    stripped_html = re.sub('<br />', ' ',lowercase)\n",
    "    cleaned_punctuation = re.sub('[%s]'%re.escape(string.punctuation),'',stripped_html)\n",
    "    return cleaned_punctuation\n",
    "\n",
    "with open(train_data_path,\"r\",encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        label,text = line.split(\"\\t\")\n",
    "        cleaned_text = clean_text(text)\n",
    "        for word in cleaned_text.split(\" \"):\n",
    "            word_count_dict[word] = word_count_dict.get(word,0)+1 \n",
    "\n",
    "df_word_dict = pd.DataFrame(pd.Series(word_count_dict,name = \"count\"))\n",
    "df_word_dict = df_word_dict.sort_values(by = \"count\",ascending =False)\n",
    "\n",
    "df_word_dict = df_word_dict[0:MAX_WORDS-2] #  \n",
    "df_word_dict[\"word_id\"] = range(2,MAX_WORDS) #编号0和1分别留给未知词<unkown>和填充<padding>\n",
    "\n",
    "word_id_dict = df_word_dict[\"word_id\"].to_dict()\n",
    "\n",
    "df_word_dict.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(data_list, pad_length):\n",
    "    padded_list = data_list.copy()\n",
    "    if len(data_list)> pad_length:\n",
    "         padded_list = data_list[-pad_length:]\n",
    "    if len(data_list)< pad_length:\n",
    "         padded_list = [1]*(pad_length-len(data_list))+data_list\n",
    "    return padded_list\n",
    "\n",
    "def text_to_token(text_file, token_file):\n",
    "    with open(text_file,\"r\",encoding = 'utf-8') as fin,\\\n",
    "      open(token_file,\"w\",encoding = 'utf-8') as fout:\n",
    "        for line in fin:\n",
    "            label,text = line.split(\"\\t\")\n",
    "            cleaned_text = clean_text(text)\n",
    "            word_token_list = [word_id_dict.get(word, 0) for word in cleaned_text.split(\" \")]\n",
    "            pad_list = pad(word_token_list,MAX_LEN)\n",
    "            out_line = label+\"\\t\"+\" \".join([str(x) for x in pad_list])\n",
    "            fout.write(out_line+\"\\n\")\n",
    "        \n",
    "text_to_token(train_data_path, train_token_path)\n",
    "text_to_token(test_data_path, test_token_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(train_samples_path):\n",
    "    os.mkdir(train_samples_path)\n",
    "    \n",
    "if not os.path.exists(test_samples_path):\n",
    "    os.mkdir(test_samples_path)\n",
    "    \n",
    "    \n",
    "def split_samples(token_path, samples_dir):\n",
    "    with open(token_path, \"r\", encoding='utf-8') as fin:\n",
    "        i = 0\n",
    "        for line in fin:\n",
    "            with open(samples_dir + \"%d.txt\" % i, \"w\", encoding=\"utf-8\") as fout:\n",
    "                fout.write(line)\n",
    "            i = i+1\n",
    "\n",
    "split_samples(train_token_path, train_samples_path)\n",
    "split_samples(test_token_path, test_samples_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['0.txt', '1.txt', '10.txt', '100.txt', '1000.txt', '10000.txt', '10001.txt', '10002.txt', '10003.txt', '10004.txt', '10005.txt', '10006.txt', '10007.txt', '10008.txt', '10009.txt', '1001.txt', '10010.txt', '10011.txt', '10012.txt', '10013.txt', '10014.txt', '10015.txt', '10016.txt', '10017.txt', '10018.txt', '10019.txt', '1002.txt', '10020.txt', '10021.txt', '10022.txt', '10023.txt', '10024.txt', '10025.txt', '10026.txt', '10027.txt', '10028.txt', '10029.txt', '1003.txt', '10030.txt', '10031.txt', '10032.txt', '10033.txt', '10034.txt', '10035.txt', '10036.txt', '10037.txt', '10038.txt', '10039.txt', '1004.txt', '10040.txt', '10041.txt', '10042.txt', '10043.txt', '10044.txt', '10045.txt', '10046.txt', '10047.txt', '10048.txt', '10049.txt', '1005.txt', '10050.txt', '10051.txt', '10052.txt', '10053.txt', '10054.txt', '10055.txt', '10056.txt', '10057.txt', '10058.txt', '10059.txt', '1006.txt', '10060.txt', '10061.txt', '10062.txt', '10063.txt', '10064.txt', '10065.txt', '10066.txt', '10067.txt', '10068.txt', '10069.txt', '1007.txt', '10070.txt', '10071.txt', '10072.txt', '10073.txt', '10074.txt', '10075.txt', '10076.txt', '10077.txt', '10078.txt', '10079.txt', '1008.txt', '10080.txt', '10081.txt', '10082.txt', '10083.txt', '10084.txt', '10085.txt', '10086.txt']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(train_samples_path)[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class imdbDataset(Dataset):\n",
    "    def __init__(self, samples_dir):\n",
    "        self.samples_dir = samples_dir\n",
    "        self.samples_paths = os.listdir(samples_dir)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.samples_dir + self.samples_paths[index]\n",
    "        with open(path, \"r\", encoding = \"utf-8\") as f:\n",
    "            line = f.readline()\n",
    "            label,tokens = line.split(\"\\t\")\n",
    "            label = torch.tensor([float(label)], dtype=torch.float)\n",
    "            feature = torch.tensor([int(x) for x in tokens.split(\" \")], dtype=torch.long)\n",
    "            return (feature, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = imdbDataset(train_samples_path)\n",
    "ds_test = imdbDataset(test_samples_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20000\n5000\n"
     ]
    }
   ],
   "source": [
    "print(len(ds_train))\n",
    "print(len(ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 461,  410,    2,  ...,    8,    8,    8],\n        [   1,    1,    1,  ...,   56, 1544,    8],\n        [   1,    1,    1,  ...,    2,  126,    8],\n        ...,\n        [   1,    1,    1,  ...,   10,  171,    8],\n        [   1,    1,    1,  ..., 6415,  358,    8],\n        [  54, 2538,   46,  ...,  710,   13,    8]])\ntensor([[0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.]])\n"
     ]
    }
   ],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "for features, labels in dl_train:\n",
    "    print(features)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (embedding): Embedding(10000, 3, padding_idx=1)\n  (conv): Sequential(\n    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (relu_1): ReLU()\n    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (relu_2): ReLU()\n  )\n  (dense): Sequential(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n    (linear): Linear(in_features=6144, out_features=1, bias=True)\n    (sigmoid): Sigmoid()\n  )\n)\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n         Embedding-1               [-1, 200, 3]          30,000\n            Conv1d-2              [-1, 16, 196]             256\n         MaxPool1d-3               [-1, 16, 98]               0\n              ReLU-4               [-1, 16, 98]               0\n            Conv1d-5              [-1, 128, 97]           4,224\n         MaxPool1d-6              [-1, 128, 48]               0\n              ReLU-7              [-1, 128, 48]               0\n           Flatten-8                 [-1, 6144]               0\n            Linear-9                    [-1, 1]           6,145\n          Sigmoid-10                    [-1, 1]               0\n================================================================\nTotal params: 40,625\nTrainable params: 40,625\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.000763\nForward/backward pass size (MB): 0.287796\nParams size (MB): 0.154972\nEstimated Total Size (MB): 0.443531\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import importlib \n",
    "from torchkeras import Model, summary\n",
    "\n",
    "\n",
    "class Net(Model):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #设置padding_idx参数后将在训练过程中将填充的token始终赋值为0向量\n",
    "        self.embedding = nn.Embedding(num_embeddings=MAX_WORDS, embedding_dim=3, padding_idx=1)\n",
    "        self.conv = nn.Sequential()\n",
    "        self.conv.add_module(\"conv_1\", nn.Conv1d(in_channels=3, out_channels=16, kernel_size=5))\n",
    "        self.conv.add_module(\"pool_1\", nn.MaxPool1d(kernel_size=2))\n",
    "        self.conv.add_module(\"relu_1\", nn.ReLU())\n",
    "        self.conv.add_module(\"conv_2\", nn.Conv1d(in_channels=16, out_channels=128, kernel_size=2))\n",
    "        self.conv.add_module(\"pool_2\", nn.MaxPool1d(kernel_size=2))\n",
    "        self.conv.add_module(\"relu_2\", nn.ReLU())\n",
    "        \n",
    "        self.dense = nn.Sequential()\n",
    "        self.dense.add_module(\"flatten\", nn.Flatten())\n",
    "        self.dense.add_module(\"linear\", nn.Linear(6144, 1))\n",
    "        self.dense.add_module(\"sigmoid\", nn.Sigmoid())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x).transpose(1, 2)\n",
    "        x = self.conv(x)\n",
    "        y = self.dense(x)\n",
    "        return y\n",
    "        \n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "model.summary(input_shape=(200, ), input_dtype=torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    y_pred = torch.where(y_pred > 0.5, \\\n",
    "        torch.ones_like(y_pred, dtype=torch.float32), torch.zeros_like(y_pred, dtype=torch.float32))\n",
    "    acc = torch.mean(1 - torch.abs(y_true - y_pred))\n",
    "    return acc\n",
    "\n",
    "model.compile(loss_func=nn.BCELoss(), optimizer=torch.optim.Adagrad(model.parameters(), lr=0.02), \\\n",
    "    metrics_dict={\"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start Training ...\n",
      "\n",
      "================================================================================2020-12-08 17:48:02\n",
      "{'step': 200, 'loss': 0.726, 'accuracy': 0.511}\n",
      "{'step': 400, 'loss': 0.708, 'accuracy': 0.521}\n",
      "{'step': 600, 'loss': 0.701, 'accuracy': 0.53}\n",
      "{'step': 800, 'loss': 0.693, 'accuracy': 0.545}\n",
      "{'step': 1000, 'loss': 0.686, 'accuracy': 0.558}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   1   | 0.686 |  0.558   |  0.661   |    0.602     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2020-12-08 17:48:39\n",
      "{'step': 200, 'loss': 0.619, 'accuracy': 0.655}\n",
      "{'step': 400, 'loss': 0.607, 'accuracy': 0.667}\n",
      "{'step': 600, 'loss': 0.599, 'accuracy': 0.673}\n",
      "{'step': 800, 'loss': 0.594, 'accuracy': 0.678}\n",
      "{'step': 1000, 'loss': 0.591, 'accuracy': 0.682}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   2   | 0.591 |  0.682   |  0.575   |    0.705     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2020-12-08 17:49:15\n",
      "{'step': 200, 'loss': 0.533, 'accuracy': 0.729}\n",
      "{'step': 400, 'loss': 0.528, 'accuracy': 0.735}\n",
      "{'step': 600, 'loss': 0.527, 'accuracy': 0.739}\n",
      "{'step': 800, 'loss': 0.523, 'accuracy': 0.74}\n",
      "{'step': 1000, 'loss': 0.52, 'accuracy': 0.742}\n",
      "\n",
      " +-------+------+----------+----------+--------------+\n",
      "| epoch | loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+------+----------+----------+--------------+\n",
      "|   3   | 0.52 |  0.742   |  0.564   |    0.717     |\n",
      "+-------+------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2020-12-08 17:49:45\n",
      "{'step': 200, 'loss': 0.483, 'accuracy': 0.767}\n",
      "{'step': 400, 'loss': 0.474, 'accuracy': 0.772}\n",
      "{'step': 600, 'loss': 0.474, 'accuracy': 0.774}\n",
      "{'step': 800, 'loss': 0.472, 'accuracy': 0.776}\n",
      "{'step': 1000, 'loss': 0.469, 'accuracy': 0.777}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   4   | 0.469 |  0.777   |  0.511   |    0.757     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2020-12-08 17:50:15\n",
      "{'step': 200, 'loss': 0.424, 'accuracy': 0.801}\n",
      "{'step': 400, 'loss': 0.428, 'accuracy': 0.801}\n",
      "{'step': 600, 'loss': 0.43, 'accuracy': 0.801}\n",
      "{'step': 800, 'loss': 0.428, 'accuracy': 0.803}\n",
      "{'step': 1000, 'loss': 0.429, 'accuracy': 0.801}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   5   | 0.429 |  0.801   |  0.503   |    0.759     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2020-12-08 17:50:46\n",
      "{'step': 200, 'loss': 0.394, 'accuracy': 0.83}\n",
      "{'step': 400, 'loss': 0.394, 'accuracy': 0.825}\n",
      "{'step': 600, 'loss': 0.395, 'accuracy': 0.825}\n",
      "{'step': 800, 'loss': 0.395, 'accuracy': 0.823}\n",
      "{'step': 1000, 'loss': 0.395, 'accuracy': 0.823}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   6   | 0.395 |  0.823   |  0.497   |    0.769     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2020-12-08 17:51:17\n",
      "{'step': 200, 'loss': 0.38, 'accuracy': 0.829}\n",
      "{'step': 400, 'loss': 0.372, 'accuracy': 0.838}\n",
      "{'step': 600, 'loss': 0.368, 'accuracy': 0.839}\n",
      "{'step': 800, 'loss': 0.368, 'accuracy': 0.839}\n",
      "{'step': 1000, 'loss': 0.367, 'accuracy': 0.839}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   7   | 0.367 |  0.839   |  0.482   |    0.779     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2020-12-08 17:51:49\n",
      "{'step': 200, 'loss': 0.345, 'accuracy': 0.848}\n",
      "{'step': 400, 'loss': 0.343, 'accuracy': 0.853}\n",
      "{'step': 600, 'loss': 0.344, 'accuracy': 0.851}\n",
      "{'step': 800, 'loss': 0.345, 'accuracy': 0.85}\n",
      "{'step': 1000, 'loss': 0.342, 'accuracy': 0.852}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   8   | 0.342 |  0.852   |  0.474   |    0.784     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2020-12-08 17:52:21\n",
      "{'step': 200, 'loss': 0.328, 'accuracy': 0.859}\n",
      "{'step': 400, 'loss': 0.319, 'accuracy': 0.866}\n",
      "{'step': 600, 'loss': 0.321, 'accuracy': 0.863}\n",
      "{'step': 800, 'loss': 0.323, 'accuracy': 0.861}\n",
      "{'step': 1000, 'loss': 0.321, 'accuracy': 0.862}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   9   | 0.321 |  0.862   |  0.474   |     0.79     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2020-12-08 17:52:52\n",
      "{'step': 200, 'loss': 0.298, 'accuracy': 0.874}\n",
      "{'step': 400, 'loss': 0.304, 'accuracy': 0.873}\n",
      "{'step': 600, 'loss': 0.304, 'accuracy': 0.874}\n",
      "{'step': 800, 'loss': 0.301, 'accuracy': 0.875}\n",
      "{'step': 1000, 'loss': 0.303, 'accuracy': 0.873}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   10  | 0.303 |  0.873   |  0.468   |    0.793     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2020-12-08 17:53:29\n",
      "Finished Training...\n"
     ]
    }
   ],
   "source": [
    "dfhistory = model.fit(10, dl_train, dl_val=dl_test, log_step_freq=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.685750   0.55795  0.660555        0.6016\n",
       "1  0.591386   0.68210  0.574622        0.7048\n",
       "2  0.520254   0.74205  0.564227        0.7172\n",
       "3  0.469431   0.77670  0.510692        0.7566\n",
       "4  0.429008   0.80130  0.503139        0.7586\n",
       "5  0.394574   0.82295  0.497348        0.7686\n",
       "6  0.366846   0.83865  0.481817        0.7794\n",
       "7  0.342159   0.85185  0.474267        0.7842\n",
       "8  0.321482   0.86230  0.473824        0.7904\n",
       "9  0.302618   0.87335  0.468441        0.7930"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>accuracy</th>\n      <th>val_loss</th>\n      <th>val_accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.685750</td>\n      <td>0.55795</td>\n      <td>0.660555</td>\n      <td>0.6016</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.591386</td>\n      <td>0.68210</td>\n      <td>0.574622</td>\n      <td>0.7048</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.520254</td>\n      <td>0.74205</td>\n      <td>0.564227</td>\n      <td>0.7172</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.469431</td>\n      <td>0.77670</td>\n      <td>0.510692</td>\n      <td>0.7566</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.429008</td>\n      <td>0.80130</td>\n      <td>0.503139</td>\n      <td>0.7586</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.394574</td>\n      <td>0.82295</td>\n      <td>0.497348</td>\n      <td>0.7686</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.366846</td>\n      <td>0.83865</td>\n      <td>0.481817</td>\n      <td>0.7794</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.342159</td>\n      <td>0.85185</td>\n      <td>0.474267</td>\n      <td>0.7842</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.321482</td>\n      <td>0.86230</td>\n      <td>0.473824</td>\n      <td>0.7904</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.302618</td>\n      <td>0.87335</td>\n      <td>0.468441</td>\n      <td>0.7930</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "dfhistory"
   ]
  }
 ]
}