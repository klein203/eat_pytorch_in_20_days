{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCEWithLogitsLoss 与 BCELoss\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(7)\n",
    "\n",
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fc1 = nn.Linear(6, 1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # logits\n",
    "        return self.fc1(input)\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ac1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.ac1(input)\n",
    "\n",
    "\n",
    "N = 10\n",
    "\n",
    "X = torch.rand((N, 6))\n",
    "y = torch.where(torch.rand((N, 1)) >= 0.5, 1., 0.)\n",
    "\n",
    "net1 = Net1()\n",
    "logits = net1(X)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss = loss_fn(logits, y)\n",
    "print(loss)\n",
    "\n",
    "net2 = Net2()\n",
    "y_pred = net2(logits)\n",
    "loss_fn = nn.BCELoss()\n",
    "loss = loss_fn(y_pred, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss 与 NLLLoss\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "C = 3\n",
    "torch.manual_seed(7)\n",
    "\n",
    "\n",
    "class Net3(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fc1 = nn.Linear(6, C)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # logits - fc output\n",
    "        return self.fc1(input)\n",
    "\n",
    "class Net4(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ac1 = nn.LogSoftmax()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.ac1(input)\n",
    "\n",
    "\n",
    "X = torch.rand((N, 6))\n",
    "y = (torch.rand(N) * C).long()\n",
    "\n",
    "net3 = Net3()\n",
    "logits = net3(X)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, y)\n",
    "print(loss)\n",
    "\n",
    "net4 = Net4()\n",
    "y_pred = net4(logits)\n",
    "loss_fn = nn.NLLLoss()\n",
    "loss = loss_fn(y_pred, y)\n",
    "print(loss)"
   ]
  },
  {
   "source": [
    "# cosine similarity\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "N = 10\n",
    "torch.manual_seed(7)\n",
    "\n",
    "\n",
    "a = torch.randn((N, 6))\n",
    "b = torch.randn((N, 6))\n",
    "\n",
    "loss_fn = nn.CosineSimilarity()\n",
    "loss = loss_fn(a, b)\n",
    "loss"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "import torchkeras \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "#正负样本数量\n",
    "n_positive,n_negative = 200,6000\n",
    "\n",
    "#生成正样本, 小圆环分布\n",
    "r_p = 5.0 + torch.normal(0.0,1.0,size = [n_positive,1]) \n",
    "theta_p = 2*np.pi*torch.rand([n_positive,1])\n",
    "Xp = torch.cat([r_p*torch.cos(theta_p),r_p*torch.sin(theta_p)],axis = 1)\n",
    "Yp = torch.ones_like(r_p)\n",
    "\n",
    "#生成负样本, 大圆环分布\n",
    "r_n = 8.0 + torch.normal(0.0,1.0,size = [n_negative,1]) \n",
    "theta_n = 2*np.pi*torch.rand([n_negative,1])\n",
    "Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = 1)\n",
    "Yn = torch.zeros_like(r_n)\n",
    "\n",
    "#汇总样本\n",
    "X = torch.cat([Xp,Xn],axis = 0)\n",
    "Y = torch.cat([Yp,Yn],axis = 0)\n",
    "\n",
    "\n",
    "#可视化\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.scatter(Xp[:,0],Xp[:,1],c = \"r\")\n",
    "plt.scatter(Xn[:,0],Xn[:,1],c = \"g\")\n",
    "plt.legend([\"positive\",\"negative\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self,gamma=2.0,alpha=0.75):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self,y_pred,y_true):\n",
    "        bce = torch.nn.BCELoss(reduction = \"none\")(y_pred,y_true)\n",
    "        p_t = (y_true * y_pred) + ((1 - y_true) * (1 - y_pred))\n",
    "        alpha_factor = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n",
    "        modulating_factor = torch.pow(1.0 - p_t, self.gamma)\n",
    "        loss = torch.mean(alpha_factor * modulating_factor * bce)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel(torchkeras.Model):\n",
    "    def __init__(self):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,4)\n",
    "        self.fc2 = nn.Linear(4,8) \n",
    "        self.fc3 = nn.Linear(8,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        y = nn.Sigmoid()(self.fc3(x))\n",
    "        return y\n",
    "        \n",
    "model = DNNModel()\n",
    "\n",
    "model.summary(input_shape =(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TensorDataset(X,Y)\n",
    "\n",
    "ds_train,ds_valid = torch.utils.data.random_split(ds,[int(len(ds)*0.7),len(ds)-int(len(ds)*0.7)])\n",
    "dl_train = DataLoader(ds_train,batch_size = 100,shuffle=True)\n",
    "dl_valid = DataLoader(ds_valid,batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准确率\n",
    "def accuracy(y_pred,y_true):\n",
    "    y_pred = torch.where(y_pred>0.5,torch.ones_like(y_pred,dtype = torch.float32),\n",
    "                      torch.zeros_like(y_pred,dtype = torch.float32))\n",
    "    acc = torch.mean(1-torch.abs(y_true-y_pred))\n",
    "    return acc\n",
    "\n",
    "# L2正则化\n",
    "def L2Loss(model,alpha):\n",
    "    l2_loss = torch.tensor(0.0, requires_grad=True)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name: #一般不对偏置项使用正则\n",
    "            l2_loss = l2_loss + (0.5 * alpha * torch.sum(torch.pow(param, 2)))\n",
    "    return l2_loss\n",
    "\n",
    "# L1正则化\n",
    "def L1Loss(model,beta):\n",
    "    l1_loss = torch.tensor(0.0, requires_grad=True)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:\n",
    "            l1_loss = l1_loss +  beta * torch.sum(torch.abs(param))\n",
    "    return l1_loss\n",
    "\n",
    "# 将L2正则和L1正则添加到FocalLoss损失，一起作为目标函数\n",
    "def focal_loss_with_regularization(y_pred,y_true):\n",
    "    focal = FocalLoss()(y_pred,y_true) \n",
    "    l2_loss = L2Loss(model,0.001) #注意设置正则化项系数\n",
    "    l1_loss = L1Loss(model,0.001)\n",
    "    total_loss = focal + l2_loss + l1_loss\n",
    "    return total_loss\n",
    "\n",
    "model.compile(loss_func =focal_loss_with_regularization,\n",
    "              optimizer= torch.optim.Adam(model.parameters(),lr = 0.01),\n",
    "             metrics_dict={\"accuracy\":accuracy})\n",
    "\n",
    "dfhistory = model.fit(30,dl_train = dl_train,dl_val = dl_valid,log_step_freq = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果可视化\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize = (12,5))\n",
    "ax1.scatter(Xp[:,0],Xp[:,1], c=\"r\")\n",
    "ax1.scatter(Xn[:,0],Xn[:,1],c = \"g\")\n",
    "ax1.legend([\"positive\",\"negative\"]);\n",
    "ax1.set_title(\"y_true\");\n",
    "\n",
    "Xp_pred = X[torch.squeeze(model.forward(X)>=0.5)]\n",
    "Xn_pred = X[torch.squeeze(model.forward(X)<0.5)]\n",
    "\n",
    "ax2.scatter(Xp_pred[:,0],Xp_pred[:,1],c = \"r\")\n",
    "ax2.scatter(Xn_pred[:,0],Xn_pred[:,1],c = \"g\")\n",
    "ax2.legend([\"positive\",\"negative\"]);\n",
    "ax2.set_title(\"y_pred\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhistory"
   ]
  }
 ]
}