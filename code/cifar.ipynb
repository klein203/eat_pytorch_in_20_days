{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('py38_cv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ba15a1d979e8ac337d80e71852c3e63572e72a2fa439e8c45e7ad782aa6c2900"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### packages and globe settings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, utils, models\n",
    "from torch.utils import data\n",
    "from torchkeras import summary, Model\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "CIFAR_ROOT = os.path.join('..', 'data')\n",
    "CIFAR10_PATH = os.path.join(CIFAR_ROOT, 'cifar-10-batches-py')\n",
    "\n",
    "HISTORY_FILE = os.path.join(CIFAR10_PATH, 'cifar10_history.csv')\n",
    "WEIGHT_FILE = os.path.join(CIFAR10_PATH, 'cifar10_weight.pth')\n",
    "\n",
    "HISTORY1_FILE = os.path.join(CIFAR10_PATH, 'cifar10_history1.csv')\n",
    "HISTORY2_FILE = os.path.join(CIFAR10_PATH, 'cifar10_history2.csv')\n",
    "\n",
    "WEIGHT1_FILE = os.path.join(CIFAR10_PATH, 'cifar10_weight1.pth')\n",
    "WEIGHT2_FILE = os.path.join(CIFAR10_PATH, 'cifar10_weight2.pth')\n",
    "\n",
    "NB_CLASSES = 10\n",
    "NROWS = 8\n",
    "\n",
    "IMAGE_MEAN = 0.5\n",
    "IMAGE_STD = 0.5\n",
    "IMAGE_SIZE = 32\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = 64\n",
    "LR = 1e-2"
   ]
  },
  {
   "source": [
    "### common codes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "def plot_metric(dfhistory, metric):\n",
    "    train_metrics = dfhistory[metric]\n",
    "    val_metrics = dfhistory['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'valid_'+metric])\n",
    "    plt.show()\n",
    "\n",
    "def plot_images(features, mean=0.5, std=0.5, nrows=8, figsize=(2, 2)):\n",
    "    # images: tensor (B, C, H, W), grid_image: ndarray (C, H, W)\n",
    "    grid_image = utils.make_grid(features, nrow=nrows).numpy()\n",
    "    grid_image = mean + grid_image * std\n",
    "\n",
    "    # imshow (H, W, C)\n",
    "    grid_image = grid_image.transpose(1, 2, 0)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(grid_image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "# save and load\n",
    "def save_history(model, file, mode='csv'):\n",
    "    assert mode == 'csv'\n",
    "    assert type(model.history) is pd.DataFrame\n",
    "    model.history.to_csv(file)\n",
    "\n",
    "def save_weight(model, file):\n",
    "    weights = dict()\n",
    "    weights.update({'epoch': model.epoch})\n",
    "    weights.update({'net': model.state_dict()})\n",
    "    weights.update({'optimizer': model.optim.state_dict()})\n",
    "    torch.save(weights, file)\n",
    "\n",
    "def load_history(file, index_col='epoch', mode='csv'):\n",
    "    assert mode == 'csv'\n",
    "    return pd.read_csv(file, index_col=index_col)\n",
    "\n",
    "def load_weight(model, file, net_only=False):\n",
    "    weights = torch.load(file)\n",
    "    model.load_state_dict(weights['net'])\n",
    "    if not net_only:\n",
    "        model.epoch = weights.get('epoch', 0)\n",
    "        model.optim.load_state_dict(weights['optimizer'])\n",
    "    return model\n",
    "\n",
    "# metrics\n",
    "def precision_metrics(targets, labels):\n",
    "    # targets (-1, C), labels (-1)\n",
    "    y_pred = targets.data.max(1)[1].numpy()\n",
    "    y_true = labels.numpy()\n",
    "    score = precision_score(y_true, y_pred, average='macro')\n",
    "    # return (1)\n",
    "    return torch.tensor(score)\n",
    "\n",
    "def accuracy_metrics(targets, labels):\n",
    "    # targets (-1, C), labels (-1)\n",
    "    y_pred = targets.data.max(1)[1].numpy()\n",
    "    y_true = labels.numpy()\n",
    "    score = accuracy_score(y_true, y_pred)\n",
    "    # return (1)\n",
    "    return torch.tensor(score)\n",
    "\n",
    "# training functions\n",
    "def run_step(model, features, labels, train_mode=True):\n",
    "    targets = model(features)\n",
    "    \n",
    "    metrics = dict()\n",
    "    loss = model.loss_fn(targets, labels)\n",
    "    metrics.update({'%sloss' % ('' if train_mode else 'val_'): loss.item()})\n",
    "    \n",
    "    for metric_name, metric_fn in model.metrics_dict.items():\n",
    "        metric_value = metric_fn(targets, labels)\n",
    "        metrics.update({'%s%s' % ('' if train_mode else 'val_', metric_name): metric_value.item()})\n",
    "\n",
    "    loss.backward()\n",
    "    model.optim.step()\n",
    "    model.optim.zero_grad()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def run_epoch(model, dataloader, train_mode=True, log_per_steps=200):\n",
    "    metrics_epoch = dict()\n",
    "\n",
    "    model.train(train_mode)\n",
    "    for step, (features, labels) in enumerate(dataloader, 1):\n",
    "        metrics = run_step(model, features, labels, train_mode)\n",
    "\n",
    "        # # update loss_epoch (mean)\n",
    "        # loss_epoch = (step - 1) / step * loss_epoch + metric_val / step\n",
    "        # update metric_epoch (mean)\n",
    "        for metric_name, metric_val in metrics.items():\n",
    "            if metrics_epoch.get(metric_name) == None:\n",
    "                metrics_epoch[metric_name] = metric_val\n",
    "            else:\n",
    "                metrics_epoch[metric_name] = \\\n",
    "                    (step - 1) / step * metrics_epoch[metric_name] + metric_val / step\n",
    "\n",
    "        if step % log_per_steps == 0:\n",
    "            print(\" - Step %d, %s\" % (step, metrics_epoch))\n",
    "\n",
    "    return metrics_epoch\n",
    "\n",
    "def train_model(model, dataloader_train, dataloader_valid, epochs, log_per_epochs=10, log_per_steps=200):\n",
    "    print(\"==========\" * 6)\n",
    "    print(\"= Training model\")\n",
    "    \n",
    "    metrics_list = []\n",
    "    start_epoch = 1 + model.epoch\n",
    "    end_epoch = epochs + 1 + model.epoch\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        metrics = dict()\n",
    "        print(\"==========\" * 6)\n",
    "        print(\"= Epoch %d/%d @ %s\" % (epoch, end_epoch - 1, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        metrics_train = run_epoch(model, dataloader_train, train_mode=True, log_per_steps=log_per_steps)\n",
    "        metrics_valid = run_epoch(model, dataloader_valid, train_mode=False, log_per_steps=log_per_steps)\n",
    "        metrics.update({'epoch': epoch})\n",
    "        metrics.update(metrics_train)\n",
    "        metrics.update(metrics_valid)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "        model.epoch = epoch\n",
    "\n",
    "        if epoch % log_per_epochs == 0:\n",
    "            print('= %s' % metrics)\n",
    "        \n",
    "    print(\"==========\" * 6)\n",
    "    \n",
    "    model.history = pd.DataFrame(metrics_list)\n",
    "    model.history.set_index('epoch', inplace=True)\n",
    "    return model.history\n",
    "\n",
    "def predict_model(model, features):\n",
    "    model.eval()\n",
    "    targets = model(features)\n",
    "    \n",
    "    return targets.data.max(1)[1]\n",
    "\n",
    "def eval_model(model, features, labels):\n",
    "    model.eval()\n",
    "    targets = model(features)\n",
    "\n",
    "    metrics = dict()\n",
    "    for metric_name, metric_fn in model.metrics_dict.items():\n",
    "        metric_value = metric_fn(targets, labels)\n",
    "        metrics.update({metric_name: metric_value.item()})\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "source": [
    "### datasets and dataloader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets and dataloader\n",
    "data_tf = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomGrayscale(),\n",
    "    transforms.ToTensor(),  # 0~255 -> 0~1\n",
    "    transforms.Normalize(IMAGE_MEAN, IMAGE_STD) # 0~1 -> -1~1\n",
    "])\n",
    "\n",
    "ds_train = datasets.CIFAR10(CIFAR_ROOT, train=True, transform=data_tf, download=True)\n",
    "ds_valid = datasets.CIFAR10(CIFAR_ROOT, train=False, transform=data_tf, download=True)\n",
    "\n",
    "dl_train = data.DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dl_valid = data.DataLoader(ds_valid, batch_size=VAL_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "source": [
    "### batch sample plot (optional)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch sample plot\n",
    "batch_features, _ = next(iter(dl_train))\n",
    "\n",
    "plot_images(batch_features, mean=IMAGE_MEAN, std=IMAGE_STD, nrows=NROWS, figsize=(8, 8))"
   ]
  },
  {
   "source": [
    "### network class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVGG16(nn.Module):\n",
    "    def __init__(self, classes=10, *args, **kwargs):\n",
    "        super(SimpleVGG16, self).__init__(*args, **kwargs)\n",
    "        self.epoch = 0\n",
    "        \n",
    "        self.vgg16 = models.vgg16(pretrained=True)\n",
    "        self.fc1 = nn.Linear(1000, classes)\n",
    "        self.logsoftmax1 = nn.LogSoftmax(1)\n",
    "        self.train(True)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.vgg16(input)\n",
    "        input = self.fc1(input)\n",
    "        input = self.logsoftmax1(input)\n",
    "        return input\n",
    "    \n",
    "    def train(self, mode):\n",
    "        super(SimpleVGG16, self).train(mode)\n",
    "        self.vgg16.train(False)\n",
    "        self.fc1.train(mode)\n",
    "        self.logsoftmax1.train(mode)\n",
    "\n",
    "# Model(SimpleVGG16()).summary(input_shape=(3, 32, 32))"
   ]
  },
  {
   "source": [
    "### model training and save history and weights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n= Training model\n============================================================\n= Epoch 1/1 @ 2020-12-17 17:26:39\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-79ebe033a902>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# model training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_per_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_per_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# save training history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-aef05f8c4f6d>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader_train, dataloader_valid, epochs, log_per_epochs, log_per_steps)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"==========\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"= Epoch %d/%d @ %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_epoch\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mmetrics_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_per_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_per_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[0mmetrics_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_per_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_per_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-aef05f8c4f6d>\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(model, dataloader, train_mode, log_per_steps)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# # update loss_epoch (mean)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-aef05f8c4f6d>\u001b[0m in \u001b[0;36mrun_step\u001b[1;34m(model, features, labels, train_mode)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'%s%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtrain_mode\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'val_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmetric_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\envs\\py38_cv\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\envs\\py38_cv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training settings (loss, optim & metrics)\n",
    "model = SimpleVGG16(NB_CLASSES)\n",
    "model.loss_fn = nn.CrossEntropyLoss()\n",
    "model.optim = optim.Adam(model.parameters(), lr=LR)\n",
    "model.metrics_dict = {\n",
    "    'precision': precision_metrics,\n",
    "    'accuracy': accuracy_metrics\n",
    "}\n",
    "\n",
    "# model training\n",
    "history = train_model(model, dl_train, dl_valid, epochs=1, log_per_epochs=1, log_per_steps=1)\n",
    "\n",
    "# save training history\n",
    "save_history(model, HISTORY1_FILE)\n",
    "\n",
    "# save weights\n",
    "save_weight(model, WEIGHT1_FILE)"
   ]
  },
  {
   "source": [
    "### re-training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleVGG16(NB_CLASSES)\n",
    "model.loss_fn = nn.CrossEntropyLoss()\n",
    "model.optim = optim.Adam(model.parameters(), lr=LR)\n",
    "model.metrics_dict = {\n",
    "    'precision': precision_metrics,\n",
    "    'accuracy': accuracy_metrics\n",
    "}\n",
    "\n",
    "# load weights\n",
    "model = load_weight(model, WEIGHT1_FILE)\n",
    "history = train_model(model, dl_train, dl_valid, 10, log_per_epochs=1, log_per_steps=200)\n",
    "\n",
    "save_history(model, HISTORY2_FILE)\n",
    "save_weight(model, WEIGHT2_FILE)\n",
    "history"
   ]
  },
  {
   "source": [
    "### metrics plot"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = load_history(HISTORY_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history, 'precision')"
   ]
  },
  {
   "source": [
    "### prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, _ = next(iter(dl_valid))\n",
    "\n",
    "model = SimpleVGG16(NB_CLASSES)\n",
    "model = load_weight(model, WEIGHT_FILE, net_only=True)\n",
    "\n",
    "targets = predict_model(model, features)\n",
    "print(targets.numpy().reshape(-1, NROWS))\n",
    "\n",
    "plot_images(features, mean=IMAGE_MEAN, std=IMAGE_STD, nrows=NROWS)"
   ]
  },
  {
   "source": [
    "### evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(iter(dl_valid))\n",
    "\n",
    "model = SimpleVGG16(NB_CLASSES)\n",
    "model.metrics_dict = {\n",
    "    'precision': precision_metrics,\n",
    "    'accuracy': accuracy_metrics\n",
    "}\n",
    "model = load_weight(model, WEIGHT_FILE, net_only=True)\n",
    "\n",
    "metrics = eval_model(model, features, labels)\n",
    "\n",
    "# print(labels.reshape(-1, NROWS))\n",
    "# plot_images(features)\n",
    "print(metrics)"
   ]
  }
 ]
}